\documentclass[11pt]{article}

\usepackage{times} % Times New Roman font
\usepackage{graphicx} % For images
\usepackage{enumitem} % For numbered items
\usepackage{lipsum} % For dummy text
\usepackage{hyperref} % For references
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
} % For hyperlinks
\usepackage{amsmath} % For math equations
\usepackage{cleveref} % For clever references
\crefname{appendix}{Appendix}{Appendices}
\usepackage{fancyvrb} % For code display
\usepackage{fancyhdr} % For custom headers
\usepackage{pythonhighlight} % For Python code
\usepackage{float} % For image positioning
\usepackage{amsmath} % For math equations
\usepackage{xcolor} % For custom colors
\usepackage{natbib} % For bibliography
\usepackage{appendix} % For appendices
\usepackage[font={small,it}]{caption} 
% \usepackage{mwe}% For custom captions
\usepackage{listings}
\usepackage{color}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{geometry}
\usepackage{caption}
\captionsetup{font={small, sf}}
\usepackage{ctex}
\usepackage{siunitx}
\usepackage{booktabs}
\renewcommand{\tablename}{Table}
\renewcommand{\figurename}{Figure}
\renewcommand{\abstractname}{Abstract}
\geometry{a4paper, margin=1in}
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}
\usepackage{fancyhdr}
\setlength{\headheight}{13.59999pt}
\addtolength{\topmargin}{-1.59999pt}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,
    breaklines=true,
    captionpos=b,
    keepspaces=true,
    numbers=left,
    numbersep=5pt,
    showspaces=false,
    showstringspaces=false,
    showtabs=false,
    tabsize=2
}

\lstset{style=mystyle}

\lstnewenvironment{outputlisting}{
  \lstset{
    basicstyle=\ttfamily\footnotesize,
    frame=single,
    backgroundcolor=\color{lightgray},
    keywordstyle=\color{blue},
    breaklines=true,
    postbreak=\mbox{\textcolor{red}{$\hookrightarrow$}\space},
    xleftmargin=.1\textwidth,
    xrightmargin=.1\textwidth,
    captionpos=b,
    language=[Sharp]C,
    numbers=none,
    tabsize=2,
    showstringspaces=false
  }
}{}

\title{Complex Network Analysis of Classical Chinese Literature: Insights from the Confucian Canon of Scripta Sinica Corpus}
\author{Yichi Zhang u7748799}
\date{May, 2024}

\begin{document}

\maketitle

\thispagestyle{fancy}

\begin{abstract}
    The study of language 
\end{abstract}

\section{Introduction}
Complex network studies have made it possible to analyze and comprehend the dynamics and structure of different real-world systems, such as biological, social, and technical networks \cite{Albert2002,Dorogovtsev2002,Newman2003,Boccaletti2006,Costa2007}. And there is no exception in human language network. 

By representing language as a network, where words or characters are nodes and their relations are edges \cite{FerreriCancho2004,Liu2004}, researchers have found statistical features in language networks such as scale-free degree distributions, small-world effects, and hierarchical structures. These findings have not only deepened our understanding of the complexity of human language but have also provided us with new insights into language learning and processing \cite{Baronchelli2013}.

Given that it is the existing language with the longest history and the largest speaker population in the world, Chinese offers a good case for the academic study of language networks. Chinese is an ideal language to study the connection between complex network theory and linguistic principles because of its large vocabulary, special reading and writing system, and rich literary history.

However, a thorough examination of the topological characteristics of Classical Chinese language networks based on sizable corpora is still absent, in spite of some groundbreaking research on Modern Chinese networks \cite{Li2007,Liu2008}.

We investigate the statistical characteristics of Chinese language networks built from the Scripta Sinica corpus, one of the largest digital collections of ancient Chinese writings, in order to close this gap. Two types of networks are constructed: the character co-occurrence network (CCN) and the character-sentence network (CSN). Our goal is to identify the network features of the Classical Chinese language and any possible linguistic or cultural conclusions by combining quantitative investigation and qualitative interpretation.

This project makes two primary contributions. Firstly, we present a thorough examination of the degree distributions, small-world effects, hierarchical structures, disassortativity, and other pertinent aspects of the topological characteristics of Chinese language networks. In addition to confirming some general features of language networks, our results also point to several distinct patterns that might be indicative of the particular qualities of the classical Chinese language. Secondly, we present the possibilities of using network science to the quantitative analysis of classical Chinese literature and culture. We demonstrate how complex network theory may be used to digital humanities research by building and analyzing networks from extensive historical corpora, creating new opportunities for multidisciplinary cooperation.

This is how the remainder of the essay is structured. The Scripta Sinica corpus and the creation of CCN and CSN, as well as the data and methodologies employed in this investigation, are covered in Section 2. The primary findings of our investigation are shown in Section 3, with an emphasis on the statistical characteristics of the two networks. Section 4 focus on the dicsussions of our results, how they relate to previous research and the limitations of the dataset. The paper is finally concluded in Section 5, which also suggests options for future research.

\section{Data and Methods}

\subsection{Scripta Sinica Corpus}
The Scripta Sinica corpus, a comprehensive digital collection of ancient Chinese literature, is managed by the Taiwanese Academia Sinica \cite{ScriptaSinica}. From the pre-Qin era (before 221 BCE) to the Qing dynasty (1644–1912 CE), it contains an extensive range of historical works, including classical literature, historical accounts, philosophical writings, and scientific treatises. Every work in the corpus has been sorted into a distinct subset.

In this study, we focus on a subset of the Scripta Sinica corpus,  specifically the Confucian Canon (Ruzang, 儒藏). This subset, notated as SS-CC, contains more than three hundred classical works of Confucianism, covering annotations of Confucian classics by various literati in all eras. The SS-CC subset represents the canonical works of Classical Chinese literature and provides a suitable dataset for studying the language networks of Classical Chinese.

The text in SS-CC is not consistent in various aspects such as traditional and simplified Chinese, punctuation, and so on. Before we start building the network, we need to preprocess the text. Traditional characters are converted to simplified characters to maintain consistency, and then all unpunctuated text is punctuated, preprocessed, and segmented using the Jiayan model, an NLP tool developed specifically for processing Classical Chinese.

\subsection{Network Construction}
We construct two types of language networks from the SS-CC corpus, namely the character co-occurrence network (CCN) and the character-sentence network (CSN). Both networks are represented as undirected and unweighted graphs, with nodes representing Chinese characters and edges indicating their co-occurrence relationships.

\subsubsection{Character Co-occurrence Network (CCN)}
In CCN, two characters are connected by an edge if they appear adjacent to each other in at least one text in the corpus. Specifically, for each text in SS-CC, we first extract all the unique characters and then create edges between characters that are adjacent in the original text. The edges are then accumulated across all texts to form the final network. Formally, the CCN can be defined as a graph $G_1=(V_1,E_1)$, where $V_1$ is the set of unique characters and $E_1$ is the set of edges representing adjacent co-occurrences.
\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.5\textwidth]{../example/ccn_example.png}
    \caption{An example of constructing the character co-occurrence network (CCN) from a short text.}
    \label{fig:ccn_example}
    \end{figure}
Figure \ref{fig:ccn_example} illustrates the construction of CCN using a simple example. Given a short text "学而时习之,不亦说乎? 有朋自远方来,不亦乐乎? 人不知自而不愠,不亦君子乎?" (Isn't it a pleasure to study and practice what you have learned? Isn't it delightful to have friends coming from distant quarters? Isn't it characteristic of a noble person to be unaffected when others misunderstand them?), we first extract the unique characters {学,而,时,习,之,不亦,说,乎,有,朋,自,远方,来,不亦,乐,乎,人,不知,而,不愠,不亦,君子,乎} as nodes, and then create edges between adjacent characters, such as (学,而), (而,时), (时,习), (习,之), and so on. The resulting subgraph is then merged into the global CCN.

\subsubsection{Character-Sentence Network (CSN)}
In CSN, two characters are connected by an edge if they co-occur in the same sentence, regardless of their positions. For each sentence in the SS-CC corpus, we create a complete subgraph among all the unique characters in the sentence, and then merge these subgraphs to form the final network. Formally, the CSN can be defined as a graph $G_2=(V_2,E_2)$, where $V_2$ is the set of unique characters and $E_2$ is the set of edges representing sentence-level co-occurrences.
\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.5\textwidth]{../example/csn_example.png}
    \caption{An example of constructing the character-sentence network (CSN) from a set of sentences.}
    \label{fig:csn_example}
    \end{figure}
Figure \ref{fig:csn_example} shows an example of constructing CSN from three sentences: "学而时习之,不亦说乎?" (Isn't it a pleasure to study and practice what you have learned?), "有朋自远方来,不亦乐乎?" (Isn't it delightful to have friends coming from distant quarters?) and "人不知自而不愠,不亦君子乎?"(Isn't it characteristic of a noble person to be unaffected when others misunderstand them?). For each sentence, a complete subgraph is created among its characters. These subgraphs are then merged to form the global CSN.

The basic statistics of the two networks are summarized in Table \ref{tab:network_stats} and the statistics of corresponding random networks are in Table \ref{tab:control_group_stats}. CCN contains 4,253 nodes and 3,119,616 edges, with an average degree of 39.77. CSN is smaller but denser, with 74,837 nodes and 2,766,801 edges, and an average degree of 73.96. Due to arithmetic limitations, we are unable to directly compute the clustering coefficients and average shortest path lengths (When the network is not connected, it is not possible to calculate the average shortest path length. So we calculate the average shortest path length of only the largest connected subgraph of the network), that is why we use sampling to calculate. In the table following, we uniformly set the sampling ratio to 0.01 and the number of samples to 100. 
\begin{table}[htbp]
    \centering
    \caption{Basic statistics of CCN and CSN}
    \label{tab:network_stats}
    \begin{tabular}{|p{6cm}|c|c|}
        \hline
        \toprule
        Graph & {CCN} & {CSN} \\
        \midrule
        N (Number of nodes) & 156879 & 74837 \\
        E (Number of edges) & 3119616 & 2766801 \\
        $\langle k \rangle$ (Average degree) & 39.77 & 73.96 \\
        C (Clustering coefficient) & 0.022 & 0.046 \\
        L (Average shortest path length) & 2.41 & 2.74 \\
        \bottomrule
    \end{tabular}
\end{table}

\begin{table}[htbp]
    \centering
    \caption{Basic statistics of control group}
    \label{tab:control_group_stats}
    \begin{tabular}{|p{6cm}|c|c|}
        \hline
        \toprule
        Property & {CCN} & {CSN} \\
        \midrule
        N (Number of nodes) & 156879 & 74837 \\
        E (Number of edges) & 3119616 & 2766801 \\
        $\langle k \rangle$ (Average degree) & 39.77 & 73.96 \\
        C (Clustering coefficient) & $2.83 \times 10^{-5}$ & $1.20 \times 10^{-4}$ \\\\
        L (Average shortest path length) & 3.66 & 2.93 \\\\
        \bottomrule
    \end{tabular}
\end{table}
\subsection{Network Analysis Methods}
To analyze the constructed networks, we employ a range of network measures and techniques commonly used in complex network analysis. These include:

\begin{itemize}
    \item Node Strength: Although we analyze unweighted networks, we also introduce and analyze the concept of node strength. Node strength is related to edge weights and represents the sum of the weights of all edges connected to a node. If we define the weight of an edge between two nodes as the frequency with which they appear adjacent to each other in CCN or simultaneously in a sentence in CSN, then the strength of a node corresponds to its total word frequency.
    By analyzing node strength, we discover a nontrivial power-law relationship between node strength $s$ and degree $k$: $s \sim k^{\beta}$. This correlation demonstrates the existence of the ``rich get richer'' phenomenon in the network, which may account for the power-law degree distribution observed in the unweighted networks. 
    \item Degree distribution: We examine whether the degree distributions of CCN and CSN show scale-free behavior, a characteristic that many real-world networks share. Scale-free networks are characterized by a power-law degree distribution, wherein a small number of nodes, called hubs, have a large number of connections, while the bulk of nodes have relatively few connections. Power-law distributions have a major impact on information transmission and network resilience and are an indication of heterogeneous network architecture.
    \item Clustering coefficient: And we look at is the networks' clustering coefficient, which indicates how likely it is for nodes to form clusters. We may determine if the networks have a hierarchical structure—that is, node clusters arranged in a multi-level manner—by examining the correlation between the clustering coefficient and node degree. Within language networks, the presence of a hierarchical structure implies the existence of meaningful sub-units.
    \item Average shortest path length: To evaluate the efficiency of information transfer and the overall connectivity of the networks, we calculate the average shortest path length. The small-world effect, characterized by small average shortest path length relative to the network size, is another feature of many real-world networks. High local clustering and short global distances are characteristics of small-world networks that facilitate effective communication within the network.
    \item Degree correlations: In addition, we investigate the average nearest neighbor degree and degree-degree correlations in order to better understand the degree correlations in CCN and CSN. The results of this study show whether the networks show assortative or disassortative mixing patterns, which represent the inclination of nodes to form connections with other nodes of correspondingly different degrees. Disassortative mixing says that high-degree nodes prefer to link with low-degree nodes, whereas assortative mixing argues that nodes with high degrees like to connect with other high-degree nodes. Understanding the fundamental structure and evolutionary history of the language networks can be gained from the mixing patterns.
\end{itemize}
    
The network analysis is performed using Python and popular network analysis libraries NetworkX. As mentioned earlier, due to computational limitations, we employed a sampling approach when calculating the relationship between the network clustering coefficient and degree, the average shortest path length of the network, and the relationship between the average nearest neighbor degree and degree. This approach allows us to maintain the overall network properties while reducing the computational burden and improving efficiency.

\section{Models and Results}
This section presents the main findings of our study on the topological properties of CCN and CSN. We focus on the small-world properties, scale-free characteristics, disassortativity and hierarchical structure. These results provide valuable insights into the organizational principles and structural features of Chinese language networks.
\subsection{Small-World Properties}

Compared to a random network with same scale, if a specific network maintains a similar average shortest path length and has a larger clustering coefficient, it is considered to have small world properties \cite{Watts1998}. Therefore, we investigated the clustering coefficient, and average shortest path length of CCN and CSN to assess their small world properties.
\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.8\textwidth]{../analysis/shortest path/shortest path.png}
    \caption{Shortest path length distributions of CCN and CSN. The average shortest path lengths are 2.41 for CCN and 2.74 for CSN.}
    \label{fig:shortest_path}
    \end{figure}
    Compared to an equivalent random network ($C_{\rm rand}=2.83 \times 10^{-5}, L_{\rm rand}=3.66$), CCN has a significantly higher clustering coefficient $C_1=0.022$ and a slightly lower average shortest path length $L_1=2.41$. Similarly, CSN has a higher $C_2=0.046$ and a lower $L_2=2.74$ ($C_{\rm rand}=1.20 \times 10^{-4}, L_{\rm rand}=2.93$). These findings suggest that Classical Chinese lexical networks have strong small-world effects, making information processing and retrieval easier due to their highly connected local clusters and efficient global communication.
\subsection{Scale-Free Characteristics and Matthew Effect}
Many real-world networks exhibit a power-law degree distribution, indicating a scale-free structure \cite{Barabasi1999}:
\begin{equation}
P(k) \sim k^{-\gamma}
\end{equation}
\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.8\textwidth]{../analysis/degree distribution/degree distribution.png}
    \caption{Degree distributions of CCN and CSN on a double logarithmic scale. The power-law exponents are 1.65 for CCN and 1.13 for CSN.}
    \label{fig:degree_dist}
    \end{figure}
    The degree distribution $P(k)$ represents the probability that a randomly selected node in the network has degree $k$. Figure \ref{fig:degree_dist} shows the degree distributions of CCN and CSN on a double logarithmic scale. Both networks exhibit power-law tails, indicating a scale-free structure.
    The scale-free nature suggests that a small number of highly connected characters (e.g., function words and common content words, which are usually single characters rather than word combinations in Classical Chinese) play a crucial role in maintaining the connectivity of the Classical Chinese language system.
    To further investigate the relationship between node importance and connection strength, we analyzed the node strength distribution and its correlation with degree.
\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.8\textwidth]{../analysis/node strength/node strenth.png}
    \caption{Average node strength $s(k)$ vs. degree $k$ for the CCN and CSN networks on a double logarithmic scale. The data is fitted to a power law $s(k) = ak^b$. For CCN, $a=2.19e-01$ and $b=1.41$, indicating a super-linear scaling of node strength with degree. For CSN, $a=4.18e-02$ and $b=1.68$, also showing a super-linear scaling but with a higher exponent compared to CCN.}
    \label{fig:strength_dist}
    \end{figure}
    Figure \ref{fig:strength_dist} shows the node strength distributions of CCN and CSN on a double logarithmic scale. Both networks exhibit heavy-tailed distributions, indicating a highly heterogeneous allocation of connection strengths among nodes. 
    Moreover, we observed a super-linear relationship between node strength $s(k)$ and degree $k$ (Figure \ref{fig:strength_dist}):
    \begin{equation}
    s(k) \sim k^{\alpha}
    \end{equation}
    For CCN, the fitted exponent is $\alpha=1.41$, while for CSN, $\alpha=1.68$. This result suggests that high-degree nodes tend to have disproportionately larger strengths compared to low-degree nodes, reflecting the preferential attachment mechanism commonly observed in evolving networks and pointing to a "Matthew effect" or "rich-get-richer" phenomenon \cite{Barabasi1999}.
\subsection{Hierarchical Structure and Disassortativity}
By observing the power-law relationship between the clustering coefficient $C(k)$ and the node degree $k$ \cite{Ravasz2003}, we can examine whether a complex network possesses a hierarchical structure:
\begin{equation}
C(k) \sim k^{-\beta}
\end{equation}
\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.8\textwidth]{../analysis/clustering coefficient/clustering coefficient.png}
    \caption{Average clustering coefficient $C(k)$ vs. degree $k$ for the CCN and CSN networks on a double logarithmic scale, using sampled data. The data is fitted to a power law $C(k) = ak^b$. For CCN, $a=2.36e+00$ and $b=-0.55$, indicating a sub-linear decrease of clustering coefficient with degree. For CSN, $a=2.05e+00$ and $b=-0.34$, also showing a sub-linear decrease but with a slower decay compared to CCN.}
    \label{fig:hierarchical}
    \end{figure}
    The hierarchical structures of CCN and CSN reflect the multi-scale, modular organization of the Chinese language system, as shown in Figure \ref{fig:hierarchical}, with $\beta=0.55$ for CCN and $\beta=0.34$ for CSN. There are tightly connected groups at various linguistic unit levels, from radicals to words and phrases.
    In addition, we studied the degree correlations in CSN and CCN to understand the assortativity of the networks. Disassortative mixing is the tendency of high-degree nodes to connect with low-degree nodes, measured by a negative degree correlation coefficient $r$ \cite{Newman2002}.
    
\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.8\textwidth]{../analysis/knn/knn.png}
    \caption{Degree correlations in CCN and CSN. The Pearson correlation coefficients are -0.2702 for CCN and -0.6054 for CSN.}
    \label{fig:degree_correlation}
    \end{figure}
    Based on the Pearson correlation coefficient, $r_1=-0.2702$ for CCN and $r_2=-0.6054$ for CSN. Our findings confirm the disassortative nature of CCN and CSN (Figure \ref{fig:degree_correlation}):
    \begin{equation}
    r = \frac{M^{-1}\sum_{i}j_ik_i-[M^{-1}\sum_{i}\frac{1}{2}(j_i+k_i)]^2}{M^{-1}\sum_{i}\frac{1}{2}(j_i^2+k_i^2)-[M^{-1}\sum_{i}\frac{1}{2}(j_i+k_i)]^2}
    \end{equation}
    High-degree function words typically associate with low-degree content words, a phenomenon known as disassortativity, which reflects syntactic and semantic constraints in language organization. Moreover, it may enhance the robustness of the language network against errors and attacks \cite{Newman, 2002}.
    In summary, our study shows that CCN and CSN exhibit a range of complex network characteristics, including hierarchical organization, small-world effect, scale-free structure, and disassortative mixing. These features provide a quantitative description of the Chinese language system and shed new light on its dynamics, structure, and function. 
\section{Discussion}
Our complex network analysis of the character co-occurrence network (CCN) and the character-sentence network (CSN) constructed from the Scripta Sinica corpus provides novel insights into the organizational principles and statistical regularities of the classical Chinese language system. The results reveal several non-trivial topological properties, including the scale-free structure, small-world effect, hierarchical organization, and disassortative mixing, which are consistent with the findings of previous studies on modern Chinese and other language networks \cite{FerreriCancho2001,Sole2010,Cong2014}. These properties suggest that the Chinese language system, despite its unique features and diachronic evolution, shares some universal characteristics with other human languages, reflecting the common cognitive and communicative constraints in language processing and acquisition.

Comparing our findings with the modern Chinese language networks analyzed in the paper \cite{Li2007}, we observe many similarities.  Scale-free degree distributions, small-world features, and disassortative mixing patterns are present in both classical and contemporary Chinese networks. The power-law degree distribution of the scale-free structure suggests the existence of strongly connected hub characters or key words in the language system. The small-world effect, which has short average path lengths and substantial local clustering, points to an effective structure for processing and retrieving information. High-degree nodes typically connect with low-degree nodes in disassortative mixing, which is a reflection of the syntactic and semantic limitations in language use. These shared characteristics show how the Chinese language system is both structurally sound and functionally ideal throughout time.

These can be further understood from a language evolution perspective. Grammar, vocabulary, and writing system simplifications and regularizations occurred throughout the Chinese language's transition from the classical to the modern forms \cite{Chen1999}. But the fundamental organizational ideas—like small-world and scale-free properties—have mostly held true, preserving the harmony between stability and adaptability in language processing and communication.

At the same time, we also observe some differences between the classical and modern Chinese networks. The statistical structure of language networks has changed due to the advent of vernacular Chinese, the standardization of character usage, and the influence of other languages. Based on linguistic theories, we have several speculations about the statistical structure of Classical Chinese texts.

First, due to the concise and highly contextualized nature of classical Chinese, where characters are more closely related to each other within and across sentences, Classical Chinese networks would be likely to have higher clustering coefficients and shorter average path lengths than their modern counterparts. Second, syntactic rules and semantic categories in classical Chinese are more rigid compared to the modern vernacular,  implying a stricter separation between function and content characters, so that the disassortative mixing pattern is likely to be more pronounced in the classical Chinese networks. 

However, not all inferences can be empirically verified through data analysis.

We can demonstrate the more pronounced disassortative mixing pattern in the classical Chinese networks by comparing the Pearson correlation coefficients: -0.2702 for CCN and -0.6054 for CSN, which are significantly larger in magnitude than -0.0759 for CLN1 and -0.0707 for CLN2.  But we cannot obtain the result that the clustering coefficient is higher and the average path length is shorter in the classical Chinese networks. 

The indicators that do not align with existing linguistic understanding may have several explanations:

1. Our study is based on a limited corpus of classical Chinese texts and is not comprehensive. Additionally, statistical result (Total number of characters: 36,762,062, Total number of Chinese characters: 27,868,060, Total number of noise characters: 160,150, Total noise ratio: 0.44\%) shows a significant amount of noise in the texts, which could be a source of bias.

2. Due to technical limitations, we used sampling methods for calculating multiple indicators. Although these methods can yield relatively reliable results, they are still less complete compared to full-scale calculations.

3. During text preprocessing, we were constrained by the lack of a robust NLP model for classical Chinese. Consequently, we could not create a network that accurately reflects classical Chinese word segmentation habits, leading to inaccuracies in network construction.
\section{Conclusion}

\section*{Acknowledgments}

\section*{Ethical Considerations and Broader Impacts}

\section*{References}

\appendix
\section*{Appendices}
\section*{Supplementary Materials}
\subsection*{Network Visualization}

\subsection*{Statistical Tests}


\subsection*{Code and Data Availability}


% References
% \begin{thebibliography}{9}
%   \bibitem{example}
%   Author,
%   \emph{Title}.
%   Publisher, Year.
% \end{thebibliography}
% \bibliographystyle{plain}
% \bibliography{references}  % replace with the actual filename of your .bib file, without the extension

\end{document}
